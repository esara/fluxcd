apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: alloy
  namespace: monitoring
spec:
  dependsOn:
    - name: tempo
  interval: 1h
  chart:
    spec:
      version: 1.2.1
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: grafana-charts
  values:
    controller:
      type: deployment
    alloy:
      stabilityLevel: public-preview
      extraPorts:
        - name: "grpc"
          port: 4317
          targetPort: 4317
        - name: "http"
          port: 4318
          targetPort: 4318
      configMap:
        content: |
          otelcol.exporter.otlp "tempo" {
            client {
              endpoint = "tempo.monitoring:4317"
              tls {
                insecure = true
              }
            }
          }

          prometheus.remote_write "default" {
            endpoint {
              url = "http://kube-prometheus-stack-prometheus:9090/api/v1/write"
            }
          }

          otelcol.exporter.prometheus "default" {
            forward_to = [prometheus.remote_write.default.receiver]
          }

          otelcol.processor.batch "default" {
            output {
              logs    = [otelcol.exporter.loki.default.input]
              metrics = [otelcol.exporter.prometheus.default.input]
              traces  = [otelcol.exporter.otlp.tempo.input]
            }
          }

          otelcol.processor.k8sattributes "default" {
            extract {
              label {
                from = "pod"
              }
          
              metadata = [
                "k8s.namespace.name",
                "k8s.pod.name",
                "k8s.pod.uid",
                "k8s.deployment.name",
                "k8s.node.name",
                "k8s.pod.start_time",
                "container.id",
              ]
            }

            output {
              metrics  = [otelcol.processor.batch.default.input]
              traces  = [otelcol.processor.batch.default.input]
            }
          }

          otelcol.receiver.otlp "default" {
            grpc {
              endpoint = "0.0.0.0:4317"
            }
            http {
              endpoint = "0.0.0.0:4318"
            }

            output {
              logs    = [otelcol.processor.batch.default.input]
              metrics = [otelcol.processor.k8sattributes.default.input]
              traces  = [otelcol.processor.k8sattributes.default.input]
            }
          }

          // discovery.kubernetes allows you to find scrape targets from Kubernetes resources.
          // It watches cluster state and ensures targets are continually synced with what is currently running in your cluster.
          discovery.kubernetes "pod_logs" {
            role = "pod"
          }
    
          // discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
          // If no rules are defined, then the input targets are exported as-is.
          discovery.relabel "pod_logs" {
            targets = discovery.kubernetes.pod_logs.targets
          
            // Label creation - "namespace" field from "__meta_kubernetes_namespace"
            rule {
              source_labels = ["__meta_kubernetes_namespace"]
              action = "replace"
              target_label = "namespace"
            }
    
            // Label creation - "pod" field from "__meta_kubernetes_pod_name"
            rule {
              source_labels = ["__meta_kubernetes_pod_name"]
              action = "replace"
              target_label = "pod"
            }
    
            // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
            rule {
              source_labels = ["__meta_kubernetes_pod_container_name"]
              action = "replace"
              target_label = "container"
            }
    
            // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
            rule {
              source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
              action = "replace"
              target_label = "app"
            }
    
            // Label creation -  "job" field from "__meta_kubernetes_namespace" and "__meta_kubernetes_pod_container_name"
            // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name
            rule {
              source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
              action = "replace"
              target_label = "job"
              separator = "/"
              replacement = "$1"
            }
    
            // Label creation - "container" field from "__meta_kubernetes_pod_uid" and "__meta_kubernetes_pod_container_name"
            // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log
            rule {
              source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
              action = "replace"
              target_label = "__path__"
              separator = "/"
              replacement = "/var/log/pods/*$1/*.log"
            }
    
            // Label creation -  "container_runtime" field from "__meta_kubernetes_pod_container_id"
            rule {
              source_labels = ["__meta_kubernetes_pod_container_id"]
              action = "replace"
              target_label = "container_runtime"
              regex = "^(\\S+):\\/\\/.+$"
              replacement = "$1"
            }
          }
    
          loki.write "default" {
            endpoint {
              url = "http://loki:3100/loki/api/v1/push"
            }
          }

          otelcol.exporter.loki "default" {
            forward_to = [loki.write.default.receiver]
          }

          // loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.
          loki.source.kubernetes "pod_logs" {
            targets    = discovery.relabel.pod_logs.output
            forward_to = [loki.write.default.receiver]
          }

          // https://grafana.com/docs/alloy/latest/reference/components/discovery/discovery.kubernetes/#pod-role
          discovery.kubernetes "pod_profiler" {
            role = "pod"
            selectors {
              role = "pod"
              label = "app.kubernetes.io/part-of=causely,app.kubernetes.io/name!=redis,app.kubernetes.io/name!=victoriametrics,app.kubernetes.io/name!=ui"
            }
          }

          // https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/
          discovery.relabel "pod_profiler" {
            targets = discovery.kubernetes.pod_profiler.targets
            rule {
              action = "labelmap"
              regex = "__meta_kubernetes_pod_label_app_kubernetes_io_(.+)"
              replacement = "app_kubernetes_io_${1}"
            }
            rule {
              action = "labelmap"
              regex = "__meta_(kubernetes_namespace|kubernetes_pod_name|kubernetes_pod_container_name)"
              replacement = "${1}"
            }
            rule {
              action = "labelmap"
              regex = "__meta_(kubernetes_pod_ip|kubernetes_pod_uid|kubernetes_pod_node_name)"
              replacement = "${1}"
            }
            rule {
              action = "keep"
              regex  = "webserver|beyla-profile"
              source_labels = ["__meta_kubernetes_pod_container_port_name"]
            }
          }

          pyroscope.write "default" {
            endpoint {
              url = "http://pyroscope:4040"
            }
          }

          pyroscope.scrape "causely" {
            targets    = discovery.relabel.pod_profiler.output
            forward_to = [pyroscope.write.default.receiver]
            profiling_config {
              profile.memory {
                enabled = true
              }
              profile.block {
                enabled = true
              }
              profile.goroutine {
                enabled = true
              }
              profile.mutex {
                enabled = true
              }
              profile.process_cpu {
                enabled = true
              }
              profile.fgprof {
                enabled = false
              }
            }
          }